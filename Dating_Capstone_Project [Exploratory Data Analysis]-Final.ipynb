{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File cc_capstone/profiles.csv does not exist: 'cc_capstone/profiles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-523cc1cb1c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Make a copy of the dataset so we have all the original information as we manipulate columns and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc_capstone/profiles.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File cc_capstone/profiles.csv does not exist: 'cc_capstone/profiles.csv'"
     ]
    }
   ],
   "source": [
    "#Make a copy of the dataset so we have all the original information as we manipulate columns and values\n",
    "\n",
    "df = pd.read_csv('cc_capstone/profiles.csv')\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataframe with non-essay columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chars = df_copy[['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'height', 'income', 'job',\n",
    "                   'last_online', 'location', 'offspring', 'orientation', 'pets', 'religion', 'sex', 'sign', 'smokes', \n",
    "                   'speaks', 'status']]\n",
    "df_chars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize and get a feel for the data\n",
    "- Find how many samples exist in the dataset\n",
    "- Understand data type of each feature and how to best manipulate it to suit our interest\n",
    "(df.info is more comprehensive than df.dtypes)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_features = df_copy.columns\n",
    "print(df_features)\n",
    "df_features.size\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy.shape\n",
    "#Almost 60,000 observations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Table between numeric data -- Encode all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chars.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.height.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.income.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.income.value_counts()\n",
    "unreported = (59946 - 48442)/(59946)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With only {x:.2f}% of data NOT EQUAL to -1 & avaiable, one should DROP the feature.\".format(x = unreported))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a drop list and append as you find features that should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_drop_cols = ['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnum_df = df_chars.drop(['age', 'height', 'income'], axis = 1)\n",
    "nonnum_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat Status as Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chars.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chars.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Items for 'Status' column\n",
    "- <b/>Create</b> Single: 'single' + 'available'\n",
    "- <b/>Create</b> Taken: 'seeing someone'\n",
    "- <b/>Remove</b>: 'married' + 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions and thoughts -- in an accompanied Google Sheets\n",
    "- body_type:\n",
    "- diet:\n",
    "- drinks:\n",
    "- drugs:\n",
    "- education:\n",
    "\n",
    "- ethnicity:\n",
    "- height:\n",
    "- income:\n",
    "- job:\n",
    "- offspring:\n",
    "- orientation:\n",
    "- pets:\n",
    "- religion:\n",
    "- sex:\n",
    "- sign:\n",
    "- smokes:\n",
    "- speaks:\n",
    "- status:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the amount of missing data by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_missing_data_table(df):\n",
    "    total = df.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (df.isnull().sum()/ df.isnull().count()).sort_values(ascending = False)\n",
    "    missing_data = pd.concat([total, percent], axis = 1, keys = ['Total','Percent Missing'])\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_missing_data_table(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features with insufficient data; later build or average values for NA\n",
    "- offspring\n",
    "- diet\n",
    "- **pets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_drop_cols.append(['offspring', 'diet', 'religion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_cols = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_dat = {'Essay Summary': [\"My self summary\", \n",
    "             \"What I’m doing with my life\", \n",
    "             \"I’m really good at\",\n",
    "             \"The first thing people usually notice about me\",\n",
    "             \"Favorite books, movies, show, music, and food\",\n",
    "             \"The six things I could never do without\",\n",
    "             \"I spend a lot of time thinking about\",\n",
    "             \"On a typical Friday night I am\",\n",
    "             \"The most private thing I am willing to admit\",\n",
    "             \"You should message me if…\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_missing = draw_missing_data_table(df_copy).loc[essay_cols, :]\n",
    "essay_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_summary = pd.DataFrame(data = essay_dat, index = essay_cols)\n",
    "essay_summary = essay_summary.join(essay_missing)\n",
    "essay_summary.sort_values(by = 'Percent Missing', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe without Essay Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_essay_feats = df_copy[df_copy.columns.difference(essay_cols)]\n",
    "non_essay_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_essay_feats = non_essay_feats[non_essay_feats.columns.difference(['income', 'offspring', 'diet'])]\n",
    "non_essay_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a table to explore features you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_explore = non_essay_feats[non_essay_feats.columns.difference(['age', \n",
    "                                                                       'height', \n",
    "                                                                       'last_online', \n",
    "                                                                       'location', \n",
    "                                                                       'orientation', \n",
    "                                                                       'sex',])]\n",
    "feats_to_explore.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeats2encode_df = nonnum_df[['body_type', 'drinks', 'drugs', 'education', 'smokes']]\n",
    "catfeats2encode_df = nonnum_df[['diet', 'ethnicity', 'job', 'location', 'offspring',\n",
    "                                'orientation', 'pets', 'religion', 'sign', 'speaks' ]] \n",
    "#include cat feature whether currently in education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user_id column for each row; helps to group data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['user_id'] = range(len(df))\n",
    "df_copy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Location data\n",
    "-> City and State Features\n",
    "<br />\n",
    "then ->: California and Other States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_table = df_copy.location.str.split(pat = ',', expand = True)\n",
    "del location_table[2]\n",
    "location_table.rename(columns = {0: 'City', 1: 'State'}, inplace = True)\n",
    "print(location_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Table of the Total Survey Respondants by Unique Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(location_table['City'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = location_table.City\n",
    "(unq_city, city_count) = np.unique(cities, return_counts = True)\n",
    "unique_freq = np.asarray((unq_city, city_count)).T\n",
    "unique_freq.sort(axis = 0)\n",
    "unique_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(unique_freq)\n",
    "y.sort_values(by = 1, ascending = False).reset_index()\n",
    "y.columns = ['City', 'Survey Count']\n",
    "z = y.sort_values(by = 'Survey Count', ascending = False).reset_index(drop = True) #drop attribute removes the index\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[\"Cummulative Sum\"] = z['Survey Count'].cumsum()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Percent of Total Survey column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[\"Percent of Total Survey Respondants\"] = z['Cummulative Sum']/ z['Cummulative Sum'][197]*100\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By reducing the variety of locations by 134 one only removes 0.5% of the Survey Respondants !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neglig_data = z[z.index > 63]\n",
    "neglig_data[\"Survey Count\"].sum()\n",
    "neglig_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Count of the Unique states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states = location_table.State\n",
    "(unq_state, state_count) = np.unique(states, return_counts = True)\n",
    "unique_states = np.asarray((unq_state, state_count)).T\n",
    "unique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z = pd.DataFrame(unique_states)\n",
    "z.sort_values(by = 1, ascending = False).reset_index()\n",
    "z.columns = ['State', 'Survey Count']\n",
    "z.sort_values(by = 'Survey Count', ascending = False).reset_index(drop = True) #drop attribute removes the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Select Rows in the dataFrame that only contain California as a location'''\n",
    "non_essay_feats = non_essay_feats[non_essay_feats.location.str.contains('california')]\n",
    "non_essay_feats.location #Notice the length matches the Survey Count found above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice some Surveys are from outside the COUNTRY - not just the state:\n",
    "- United Kingdom (2)\n",
    "- Spain (2)\n",
    "- Vietnam (1)\n",
    "- Switzerland (1)\n",
    "- Netherlands (1)\n",
    "- Brittish Columbia (1)\n",
    "- Ireland (1)\n",
    "- Germany (1)<br>**TOTAL: 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeframe of the Data:\n",
    " + 2011 - 2012\n",
    " + End of June to Beginning of July (~Essentially JULY2011 - July2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['last_online'] = df_copy['last_online'].astype('string') # Change dtype to a string to convert into Datetime\n",
    "df_copy.last_online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datetime object to parse the last_online values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['last_online'] = pd.to_datetime(df['last_online'], format = \"%Y-%m-%d-%H-%M\")\n",
    "df_copy.last_online #access datetime features w/ df.feature.dt.____(month, date, time, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Table of Dates and Times that users Logged on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_table = pd.DataFrame()\n",
    "dt_table['Date'] = df_copy['last_online'].dt.date\n",
    "dt_table['Time'] = df_copy['last_online'].dt.time\n",
    "sorted = dt_table.sort_values(by = 'Date')\n",
    "sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_table['Time'] # under 24 hr time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most Frequent Hours users logged on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hours = [dt_table.Time[x].hour for x in range(len(dt_table.Time))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to count the Hour occurences from the times list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFreq(list):\n",
    "    freq_dic = {}\n",
    "    for hour in list:\n",
    "        freq_dic[hour] = list.count(hour)\n",
    "    return(freq_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = countFreq(hours)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict\n",
    "sort_hours = SortedDict(c)\n",
    "sort_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(sort_hours.keys())\n",
    "y = list(sort_hours.values())\n",
    "plt.bar(x = x, height = y, edgecolor = \"black\", tick_label = x)\n",
    "plt.title(\"Most Frequent log-in Hours for Users\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Information:\n",
    "- Dates\n",
    "    - Most Frequent months\n",
    "    - Most Frequent Weekday\n",
    "- Times\n",
    "    - Which 1/3 of the Day is most popular: Morning, Afternoon, Evening\n",
    "    - Popular time by the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = dt_table.Date\n",
    "(unq_date, date_count) = np.unique(dates, return_counts = True)\n",
    "unique_dates = np.asarray((unq_date, date_count)).T\n",
    "unique_dates[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(unique_dates, columns = ['Dates', 'Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Table sorted from Past to most Recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_order = dt.sort_values(by = 'Dates', ascending = True) #Dates from past to most recent\n",
    "dt_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot_date(x = dt_order['Dates'], y = dt_order['Frequency'])\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Last-Online Frequency')\n",
    "plt.title(\"Last-Online Frequency by Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Table sorted by most Frequent dates users logged-on: \n",
    "- *Find the table values to be consistent with the graph shown above!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.sort_values(by = 'Frequency', ascending = False).reset_index(drop = True) #Freq from HIGH to Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Growth stage of the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_dt = dt_order[0:355]\n",
    "low_freq_dt\n",
    "plt.plot_date(x = low_freq_dt['Dates'], y = low_freq_dt['Frequency'])\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Last-Online Frequency')\n",
    "plt.title(\"Last-Online Frequency by Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EssayQuestions include:\n",
    "- essay0 - My self summary\n",
    "- essay1 - What I’m doing with my life\n",
    "- essay2 - I’m really good at\n",
    "- essay3 - The first thing people usually notice about me\n",
    "- essay4 - Favorite books, movies, show, music, and food\n",
    "- essay5 - The six things I could never do without\n",
    "- essay6 - I spend a lot of time thinking about\n",
    "- essay7 - On a typical Friday night I am\n",
    "- essay8 - The most private thing I am willing to admit\n",
    "- essay9 - You should message me if…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix between Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_copy.corr()\n",
    "ax = sns.heatmap(corr, vmin = -1, vmax= 1, center = 0, cmap = sns.diverging_palette(20,220, n=200), square = True)\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(), rotation= 45, horizontalalignment = 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Bar charts for certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Ages of Survey Respondants')\n",
    "plt.hist(df_copy.age, bins = 30)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(16,70)\n",
    "\n",
    "# plt.boxplot(df_copy.age)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_copy.sex, bins = 2, align = 'mid')\n",
    "plt.xlabel('sex')\n",
    "plt.ylabel('freq')\n",
    "plt.title('Distribution of Male and Female participants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_copy.orientation, bins = 3)\n",
    "plt.xlabel('orientation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sexual Orientation in Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_copy.location, bins = 20, orientation = 'vertical')\n",
    "plt.xlabel('locations')\n",
    "plt.ylabel('freq')\n",
    "plt.title('Frequency of Responses based on Location')\n",
    "# plt.xticks(rotation = 'vertical')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to still synthesize:\n",
    "- Education\n",
    "- Job\n",
    "- Religion\n",
    "- Pets\n",
    "- Sign/ Zodiac\n",
    "- Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['education'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_edu = df_copy['education'].unique()\n",
    "unique_edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary that maps the 32 different education features to 4:\n",
    "- Graduated High School\n",
    "- Graduated College/ University\n",
    "- Graduated Masters or PhD program\n",
    "- Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = df_copy.groupby(['location', 'sex'])['sex'].count().sort_values(ascending = False)\n",
    "print(loc_data.head(60))\n",
    "loc_data.to_frame()\n",
    "print(loc_data['sex'])\n",
    "location_data = df_copy.groupby(['location'])['sex'].count().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['ethnicity'].unique()\n",
    "df_copy['ethnicity'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['ethnicity'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20,20]\n",
    "plt.rcParams['axes.labelsize'] = 50\n",
    "plt.rcParams['xtick.direction'] = 'inout'\n",
    "plt.rcParams['legend.fontsize'] = 25\n",
    "for i in init_drop_cols:\n",
    "    df_copy[init_drop_cols][i].hist()\n",
    "plt.tight_layout()\n",
    "plt.legend(init_drop_cols)\n",
    "# df_copy['offspring'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.diet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.drugs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.drinks.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.education.value_counts()\n",
    "# edu_table = pd.DataFrame(['In School', \n",
    "#                           'Graduated', \n",
    "#                           ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.ethnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.income.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.last_online.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.location.value_counts().head(50)\n",
    "# df_copy.location.value_counts().head(50).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.pets.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zodiac_table = pd.DataFrame(columns = ['Leo',\n",
    "                                       'Libra', \n",
    "                                       'Cancer', \n",
    "                                       'Virgo', \n",
    "                                       'Scorpio', \n",
    "                                       'Gemini', \n",
    "                                       'Taurus', \n",
    "                                       'Aries', \n",
    "                                       'Piscs', \n",
    "                                       'Aquarius', \n",
    "                                       'Sagittarius', \n",
    "                                       'Capricorn', \n",
    "                                       'Matters'])\n",
    "#Matters is a boolean value of 1 or 0 -- Y or N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.sign.value_counts(dropna = False) #44 value_counts /48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zodiac_dict = {'leo': 0, 'libra': 0, 'cancer': 0, 'virgo': 0, 'scorpio': 0, 'gemini': 0, 'taurus': 0, 'aries': 0, 'pisces': 0, 'aquarius': 0, 'sagittarius': 0, 'capricorn': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate a Question\n",
    "df.sign.value_counts()\n",
    "\n",
    "** Still have to drop na values for features + drop some features themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Your data - by mapping categorical data -> numerical\n",
    "\n",
    "done to drinks, smokes, and drugs columns\n",
    "\n",
    " drink_mapping = {\"not at all\": 0, \"rarely\": 1, \"socially\": 2, \"often\": 3, \"very often\": 4, \"desperately\": 5}\n",
    "\n",
    "all_data[\"drinks_code\"] = all_data.drinks.map(drink_mapping)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_names = df_copy.groupby(['location']).groups\n",
    "print(location_names.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df_copy.groupby(['location', 'sex'], as_index = False).count()\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate how many locations are in california and otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_frame = df_copy.location.to_frame()\n",
    "print(type(df_copy))\n",
    "# for i in range(len(location_frame)):\n",
    "#     cali_count = 0\n",
    "#     other = 0\n",
    "#     if location_frame.str.contains(', california', na = False):\n",
    "#         cali_count += 1\n",
    "#     else:\n",
    "#         other += 1\n",
    "# print(cali_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(location_data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_labels = df_copy.groupby(['location'])['sex']\n",
    "print(location_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(location_data, autopct = '%d%%')\n",
    "plt.legend(location_data)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
